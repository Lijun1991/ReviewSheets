# notes on markoc chains. lord help me. this shit is haaaaaaaaaarrrrrrrrrd

* what is a [markov process in the field of probability theory](https://en.wikipedia.org/wiki/Markov_chain)?
	* a stochastic (random) process that satisfies the markov property
		* markov property
			* a process satisfies the markov property if one can make predictions for the future of the process based solely on its present state just as well as one could knowing the process's full history
				* conditional on the present state of the system, its future and past states are independent

* what is the [markov property in the field of probability theory](https://en.wikipedia.org/wiki/Markov_property)?
	* a stochastic (random) process has the markov property if the conditional probability distribution of future states of the process depends only upon the present state not on the sequence of events that preceded it
	* strong markov property
		* same as markov property except that the meaning of present is defined in terms of a random variable known as a stopping time
	* markov assumption
		* used to describe a model where the markov property is assumed to hold 
			* hidden markov models have this assumption
	* markov random field
		* extends the markov property to two or more dimensions or to random variables defined for an interconnected network of items
			* ising model is an example
	* markov chain
		* a discrete-time stochastic process satisfying the markov property

* what is a [conditional probility distribution in the field of probability theory](https://en.wikipedia.org/wiki/Conditional_probability_distribution)?
	* given two jointly distributed random variables X and Y, the conditional probability distribution of Y given X is the probability distribution of Y when X is known to be a particular value

* what is [probability distribution in the field of probability theory](https://en.wikipedia.org/wiki/Probability_distribution)?
	* a mathematical function that can be thought of as providing the probabilit of occurence of different possible outcomes in an experiment
		* basically P in probability space

* what is a [markov chain in the field of probability theory](https://en.wikipedia.org/wiki/Markov_chain)?
	* a type of markov process that has either discrete state space or discrete index set
		* precise definition of a markov chain varies but always contains some sort of discrete interval
	* many applications as statistical models of real-world processes
		* studying cruise control systems in motor vehicles
		* queues or lines of customers arriving at an airport
		* exchange rates of currencies
		























